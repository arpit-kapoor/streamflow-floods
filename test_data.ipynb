{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd6f1f7-10c5-493f-9a82-0b104ebba8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import folium\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_data_from_file(data_dir):\n",
    "    timeseries_dfs = []\n",
    "    summary_dfs = []\n",
    "    \n",
    "    # Read all csv files from directory\n",
    "    # Sort files into timeseries and summary data\n",
    "    for file_path in glob.glob(data_dir + '**/*.csv', recursive=True):\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        # print(file_path, file_name)\n",
    "        df = pd.read_csv(file_path, low_memory=False) \n",
    "    \n",
    "        #skip these files\n",
    "        if file_name in ['streamflow_QualityCodes']:\n",
    "            continue\n",
    "    \n",
    "        if 'year' in df.columns:    \n",
    "            df['source'] = file_name\n",
    "            df= df[df['year'] > 1990]\n",
    "            df= df.drop_duplicates(['year','month','day'])\n",
    "            timeseries_dfs.append(df)\n",
    "        else:\n",
    "            df = df.rename({'ID':'station_id'}, axis=1)\n",
    "            df = df.set_index('station_id')\n",
    "            summary_dfs.append(df)\n",
    "    \n",
    "    timeseries_data = pd.concat(timeseries_dfs, axis=0, ignore_index=True)\n",
    "    timeseries_data['date'] = pd.to_datetime(timeseries_data[['year', 'month', 'day']])\n",
    "    timeseries_data = timeseries_data.drop(['year', 'month', 'day'], axis=1)\n",
    "    \n",
    "    summary_data = pd.concat(summary_dfs, axis=1)\n",
    "\n",
    "    return timeseries_data, summary_data\n",
    "\n",
    "\n",
    "def plot_catchments(camels_data, data_dir):\n",
    "    \n",
    "    cd_sd = camels_data.summary_data.loc[:,~camels_data.summary_data.columns.duplicated()]\n",
    "\n",
    "    # Define the list of cities and their latitudes/longitudes\n",
    "    cities = cd_sd['station_name']\n",
    "    lats = cd_sd['lat_outlet']\n",
    "    longs = cd_sd['long_outlet']\n",
    "    \n",
    "    # Generate a random priority for each city between 1 and 5\n",
    "    priority = np.random.randint(1, 6, size=len(cities))\n",
    "    state= cd_sd['state_outlet']\n",
    "    \n",
    "    # Create the DataFrame with the city data\n",
    "    data = {'cityname': cities,\n",
    "            'lats': lats,\n",
    "            'longs': longs,\n",
    "            'States': state,\n",
    "            'priority': priority}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    state_mapping = {'QLD': 1, 'NSW': 2, 'SA': 3, 'VIC': 4, 'ACT': 5, 'WA': 6, 'NT': 7, 'TAS': 8}\n",
    "    df['state_num'] = df['States'].map(state_mapping)\n",
    "\n",
    "    # Load the shapefile of Australia\n",
    "    # australia = gpd.read_file('STE_2021_AUST_SHP_GDA2020/STE_2021_AUST_GDA2020.shp')\n",
    "    shape_file = data_dir + '02_location_boundary_area/shp/bonus data/Australia_boundaries.shp'\n",
    "    australia = gpd.read_file(shape_file)\n",
    "    \n",
    "    # Define the CRS of the shapefile manually\n",
    "    australia.crs = 'epsg:7844'\n",
    "    \n",
    "    # Create a GeoDataFrame from the DataFrame of cities\n",
    "    gdf_cities = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longs, df.lats))\n",
    "    \n",
    "    # Set the CRS of the GeoDataFrame to EPSG 7844\n",
    "    # https://epsg.io/7844\n",
    "    gdf_cities.crs = 'epsg:7844'\n",
    "    \n",
    "    # Reproject the GeoDataFrame of cities to match the CRS of the shapefile\n",
    "    gdf_cities = gdf_cities.to_crs(australia.crs)\n",
    "    \n",
    "    # Perform a spatial join to link the cities to their corresponding polygons in the shapefile\n",
    "    gdf_cities = gpd.sjoin(gdf_cities, australia, predicate='within')\n",
    "    \n",
    "    # Set up the plot\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # # Define a custom dark color palette\n",
    "    custom_palette = sns.color_palette(['darkblue', 'black', 'purple',\n",
    "                                        'darkred', 'darkgreen', 'darkorange',\n",
    "                                        'brown' , 'blue'], \n",
    "                                       n_colors=len(df['state_num'].unique()))\n",
    "    \n",
    "    # Plot the cities colored by priority with adjustments\n",
    "    sns.scatterplot(ax=ax, data=gdf_cities, x='longs', y='lats', hue='States',\n",
    "                    s=15, palette=custom_palette, edgecolor='black',\n",
    "                    alpha=0.8, legend='full', zorder=2)\n",
    "    \n",
    "    \n",
    "    # Set x-axis limits\n",
    "    ax.set_xlim(110, 160)\n",
    "    \n",
    "    # Add the shapefile of Australia as a background map\n",
    "    australia.plot(ax=ax, color='lightgrey', edgecolor='white', zorder=1)\n",
    "    \n",
    "    # Set the plot title and axis labels\n",
    "    ax.set_title('Catchments across Australia')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64ea75e5-ec24-4446-b0c0-b21ab985e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PrepareData():\n",
    "    \n",
    "    def __init__(self, timeseries_data, summary_data):\n",
    "        ### Data Cleaning\n",
    "        self.timeseries_data = timeseries_data.replace(-99.99,np.nan)\n",
    "        \n",
    "        ### Feature Engineering\n",
    "        # get precipitation deficit\n",
    "        actualTransEvap_data = self.timeseries_data[self.timeseries_data['source'] == 'et_morton_actual_SILO'].drop(['source'], axis=1)\n",
    "        precipitation_data = self.timeseries_data[self.timeseries_data['source'] == 'precipitation_AWAP'].drop(['source'], axis=1)\n",
    "         \n",
    "        actualTransEvap_data = actualTransEvap_data[actualTransEvap_data['date'].isin(precipitation_data['date'])].reset_index(drop=True)\n",
    "        precipitation_data = precipitation_data[precipitation_data['date'].isin(actualTransEvap_data['date'])].reset_index(drop=True)\n",
    "        \n",
    "        self.precipitation_deficit = precipitation_data.drop(['date'], axis=1).subtract(actualTransEvap_data.drop(['date'], axis=1))\n",
    "        self.precipitation_deficit['source'] = 'precipitation_deficit'\n",
    "        self.precipitation_deficit['date'] = precipitation_data['date']\n",
    "        \n",
    "        # get flood probabilities\n",
    "        self.streamflow_data = self.timeseries_data[timeseries_data['source'] == 'streamflow_MLd_inclInfilled'].drop(['source'], axis=1)\n",
    "        self.streamflow_data = self.streamflow_data.set_index('date')\n",
    "        \n",
    "        self.flood_probabilities = self.streamflow_data.apply(self.flood_extent, axis=0)\n",
    "        self.flood_probabilities['source'] = 'flood_probabilities'\n",
    "        self.flood_probabilities['date'] = self.streamflow_data.index\n",
    "\n",
    "        self.flow_acceleration = np.abs(self.streamflow_data - self.streamflow_data.shift(1))\n",
    "        self.flood_prob_acc = self.flow_acceleration.apply(self.flood_extent, axis=0)\n",
    "        self.flood_prob_acc['source'] = 'flood_prob_acc'\n",
    "        self.flood_prob_acc['date'] = self.streamflow_data.index\n",
    "        \n",
    "        self.flood_indicator = self.flood_probabilities.applymap(lambda x: int(x <0.05) if pd.isnull(x) == False and isinstance(x, float) else x)\n",
    "        self.flood_indicator['source'] = 'flood_indicator'\n",
    "        self.flood_indicator['date'] = self.flood_probabilities['date']        \n",
    "        \n",
    "        # turn date into sin and cos function \n",
    "        date_min = np.min(self.flood_probabilities['date'])\n",
    "        year_seconds = 365.2425*24*60*60\n",
    "        year_sin = self.flood_probabilities['date'].apply(lambda x: np.sin((x-date_min).total_seconds() * (2 * np.pi / year_seconds)))\n",
    "        year_cos = self.flood_probabilities['date'].apply(lambda x: np.cos((x-date_min).total_seconds() * (2 * np.pi / year_seconds)))\n",
    "        all_stations = list(self.flood_probabilities.drop(columns=['source', 'date'], axis=1).columns) \n",
    "        \n",
    "        df_sin = []     \n",
    "        for value in year_sin:\n",
    "            df_sin.append({k:value for k in all_stations})\n",
    "            \n",
    "        df_sin = pd.DataFrame(df_sin)\n",
    "        df_sin['source'] = 'year_sin'\n",
    "        df_sin['date'] = self.flood_probabilities['date']\n",
    " \n",
    "        df_cos = []\n",
    "        for value in year_cos:\n",
    "            df_cos.append({k:value for k in all_stations})\n",
    "            \n",
    "        df_cos = pd.DataFrame(df_cos)\n",
    "        df_cos['source'] = 'year_cos'\n",
    "        df_cos['date'] = self.flood_probabilities['date']\n",
    "            \n",
    "        ### Return\n",
    "        self.timeseries_data = pd.concat([self.timeseries_data, self.precipitation_deficit, self.flood_probabilities, self.flood_prob_acc, df_sin, df_cos, self.flood_indicator], axis=0).reset_index(drop=True)\n",
    "        self.summary_data = summary_data\n",
    "        \n",
    "    def get_timeseries_data(self, source, stations):      \n",
    "        # filter by source\n",
    "        self.data_filtered = self.timeseries_data[self.timeseries_data['source'].isin(source)]\n",
    "\n",
    "        # filter by dates\n",
    "        self.data_filtered = self.data_filtered.loc[(self.data_filtered['date'] >= dt.datetime(1980, 1, 1)) & (self.data_filtered['date'] < dt.datetime(2015, 1, 1))]\n",
    "        # pivot data by station\n",
    "        self.data_filtered = self.data_filtered[['date', 'source'] + stations].pivot(index='date', columns='source', values=stations)\n",
    "        # get rows with no nan\n",
    "        # self.data_filtered = self.data_filtered[~self.data_filtered.isnull().any(axis=1)]\n",
    "\n",
    "        # Check if more than 10% of the data is missing\n",
    "        if self.data_filtered.isnull().any(axis=1).sum() > 0.1 * self.data_filtered.shape[0]:\n",
    "            print(f\"Station {stations} has more than 10% missing data. Skipping...\")\n",
    "            return None\n",
    "\n",
    "        for col in self.data_filtered.columns:\n",
    "            \n",
    "            print(f\"The {col[1]} column for station {stations} has {self.data_filtered[col].isna().sum()} missing values. Filling with previous year data...\")\n",
    "            \n",
    "            null_data = self.data_filtered[col][(self.data_filtered[col].isna()) & (self.data_filtered.index > dt.datetime(1980, 12, 31))]\n",
    "\n",
    "            station_df.loc[null_data.index, col] = self.data_filtered.loc[null_data.index - pd.offsets.Day(365), col].values\n",
    "\n",
    "            print(f\"The {col[1]} column for station {stations} has {self.data_filtered[col].isna().sum()} missing values after filling with previous year data.\")\n",
    "\n",
    "        # Interpolate missing values\n",
    "        self.data_filtered.interpolate(method='linear', inplace=True)\n",
    "        \n",
    "        return self.data_filtered\n",
    "        \n",
    "        \n",
    "    def get_data(self, source, stations):\n",
    "        summary_source = [i for i in source if i in list(self.summary_data.columns)]\n",
    "        timeseries_source = [i for i in source if i not in list(self.summary_data.columns)]\n",
    "     \n",
    "        # filter by source\n",
    "        self.data_filtered = self.timeseries_data[self.timeseries_data['source'].isin(timeseries_source)]\n",
    "        # pivot data by station\n",
    "        self.data_filtered = self.data_filtered[['date', 'source'] + stations].pivot(index='date', columns='source', values=stations)\n",
    "        # get rows with no nan\n",
    "        self.data_filtered = self.data_filtered[~self.data_filtered.isnull().any(axis=1)]\n",
    "        \n",
    "        for station in stations:\n",
    "            for variable in summary_source:\n",
    "                value = self.summary_data.loc[station][variable]\n",
    "                self.data_filtered[station, variable] = value\n",
    "        \n",
    "        return self.data_filtered.sort_index(axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_train_val_test(self, source, stations, \n",
    "                           scaled=True, target=['streamflow_MLd_inclInfilled'],\n",
    "                           start=None, end=None,\n",
    "                           discard=0.05, train=0.6, test=0.4):\n",
    "        assert 0<=discard<=1\n",
    "        assert (train + test) == 1\n",
    "     \n",
    "        summary_source = [i for i in source if i in list(self.summary_data.columns)]\n",
    "        timeseries_source = [i for i in source if i not in list(self.summary_data.columns)]        \n",
    "        \n",
    "        all_data = self.get_timeseries_data(timeseries_source, stations).loc[start:end]\n",
    "        n_rows_all = len(all_data)\n",
    "        \n",
    "        all_data_discarded = all_data.iloc[int(n_rows_all*discard):]\n",
    "        n_rows_discarded = len(all_data_discarded)\n",
    "        \n",
    "        train_df = all_data_discarded[:int(n_rows_discarded*train)]\n",
    "        test_df = all_data_discarded[-int(n_rows_discarded*(test)):]\n",
    "        \n",
    "        if scaled == True:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_df)\n",
    "            \n",
    "            scaler_test = MinMaxScaler()\n",
    "            scaler_test.fit(test_df)\n",
    "            \n",
    "            train_df = pd.DataFrame(scaler.transform(train_df), index=train_df.index, columns=train_df.columns)\n",
    "            test_df = pd.DataFrame(scaler_test.transform(test_df), index=test_df.index, columns=test_df.columns)\n",
    "\n",
    "            self.scaler = scaler\n",
    "            self.scaler_test = scaler_test\n",
    "            \n",
    "     \n",
    "        for station in stations:\n",
    "            for variable in summary_source:\n",
    "                value = self.summary_data.loc[station][variable]\n",
    "                \n",
    "                train_df[station, variable] = value                \n",
    "                test_df[station, variable] = value \n",
    "                                  \n",
    "        return train_df.sort_index(axis=1), test_df.sort_index(axis=1) \n",
    "    \n",
    "    # def flood_extent(self, streamflow_ts):\n",
    "    #     station_name = streamflow_ts.name\n",
    "\n",
    "    #     flow_data = pd.DataFrame(streamflow_ts)  \n",
    "    #     na_values = flow_data[flow_data[station_name].isna()][station_name]\n",
    "\n",
    "    #     flow_data = flow_data.dropna().sort_values(by=station_name, ascending=True).reset_index()\n",
    "    #     flow_data['probability'] = (flow_data.index + 1)/(1+len(flow_data)) \n",
    "    #     flow_data = flow_data.sort_values(by='date').drop(['date', station_name], axis=1)['probability']\n",
    "    #     flow_data = pd.concat([na_values, flow_data]).reset_index(drop=True) \n",
    "    #     flow_data.name = station_name  \n",
    "\n",
    "    #     return flow_data \n",
    "\n",
    "    def flood_extent(self, streamflow_ts):\n",
    "    \n",
    "        station_name = streamflow_ts.name\n",
    "    \n",
    "        flow_data = pd.DataFrame(streamflow_ts)\n",
    "        flow_data = flow_data.sort_values(by=station_name, ascending=True)\n",
    "    \n",
    "        flow_data.loc[~flow_data[station_name].isnull(), 'idx'] = np.arange((~flow_data[station_name].isnull()).sum())\n",
    "        flow_data.loc[:, 'prob'] =  (flow_data.idx + 1)/(1+len(flow_data))\n",
    "        \n",
    "        # flow_data.loc[flow_data[station_name].isnull(), 'prob'] = np.nan\n",
    "        flood_prob = flow_data.prob\n",
    "        flood_prob.name = station_name\n",
    "        flood_prob = flood_prob.sort_index()\n",
    "    \n",
    "        return flood_prob.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10b56933-f818-4f16-b011-a000489b1407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pbs.6390237.kman.restech.unsw.edu.au/ipykernel_4064030/3537397737.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  self.flood_indicator = self.flood_probabilities.applymap(lambda x: int(x <0.05) if pd.isnull(x) == False and isinstance(x, float) else x)\n"
     ]
    }
   ],
   "source": [
    "# Read timeseries and summary data from data dir\n",
    "data_dir = '/srv/scratch/z5370003/data/camels-dropbox/'\n",
    "timeseries_data, summary_data = read_data_from_file(data_dir)\n",
    "\n",
    "# Create Dataset\n",
    "camels_data = PrepareData(timeseries_data, summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a310211-cc3c-4c3d-bbbe-898b0381d6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precipitation_deficit column for station ['A5040517'] has 0 missing values. Filling with previous year data...\n",
      "The precipitation_deficit column for station ['A5040517'] has 0 missing values after filling with previous year data.\n",
      "The streamflow_mmd column for station ['A5040517'] has 3684 missing values. Filling with previous year data...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"[Timestamp('1990-02-20 00:00:00'), Timestamp('1990-02-21 00:00:00'), Timestamp('1990-02-22 00:00:00'), Timestamp('1990-02-23 00:00:00'), Timestamp('1990-02-24 00:00:00'), Timestamp('1990-02-25 00:00:00'), Timestamp('1990-02-26 00:00:00'), Timestamp('1990-02-27 00:00:00'), Timestamp('1990-02-28 00:00:00'), Timestamp('1990-03-01 00:00:00'), Timestamp('1990-03-02 00:00:00'), Timestamp('1990-03-03 00:00:00'), Timestamp('1990-03-04 00:00:00'), Timestamp('1990-03-05 00:00:00'), Timestamp('1990-03-06 00:00:00'), Timestamp('1990-03-07 00:00:00'), Timestamp('1990-03-08 00:00:00'), Timestamp('1990-03-09 00:00:00'), Timestamp('1990-03-10 00:00:00'), Timestamp('1990-03-11 00:00:00'), Timestamp('1990-03-12 00:00:00'), Timestamp('1990-03-13 00:00:00'), Timestamp('1990-03-14 00:00:00'), Timestamp('1990-03-15 00:00:00'), Timestamp('1990-03-16 00:00:00'), Timestamp('1990-03-17 00:00:00'), Timestamp('1990-03-18 00:00:00'), Timestamp('1990-03-19 00:00:00'), Timestamp('1990-03-20 00:00:00'), Timestamp('1990-03-21 00:00:00'), Timestamp('1990-03-22 00:00:00'), Timestamp('1990-03-23 00:00:00'), Timestamp('1990-03-24 00:00:00'), Timestamp('1990-03-25 00:00:00'), Timestamp('1990-03-26 00:00:00'), Timestamp('1990-03-27 00:00:00'), Timestamp('1990-03-28 00:00:00'), Timestamp('1990-03-29 00:00:00'), Timestamp('1990-03-30 00:00:00'), Timestamp('1990-03-31 00:00:00'), Timestamp('1990-04-01 00:00:00'), Timestamp('1990-04-02 00:00:00'), Timestamp('1990-04-03 00:00:00'), Timestamp('1990-04-04 00:00:00'), Timestamp('1990-04-05 00:00:00'), Timestamp('1990-04-06 00:00:00'), Timestamp('1990-04-07 00:00:00'), Timestamp('1990-04-08 00:00:00'), Timestamp('1990-04-09 00:00:00'), Timestamp('1990-04-10 00:00:00'), Timestamp('1990-04-11 00:00:00'), Timestamp('1990-04-12 00:00:00'), Timestamp('1990-04-13 00:00:00'), Timestamp('1990-04-14 00:00:00'), Timestamp('1990-04-15 00:00:00'), Timestamp('1990-04-16 00:00:00'), Timestamp('1990-04-17 00:00:00'), Timestamp('1990-04-18 00:00:00'), Timestamp('1990-04-19 00:00:00'), Timestamp('1990-04-20 00:00:00'), Timestamp('1990-04-21 00:00:00'), Timestamp('1990-04-22 00:00:00'), Timestamp('1990-04-23 00:00:00'), Timestamp('1990-04-24 00:00:00'), Timestamp('1990-04-25 00:00:00'), Timestamp('1990-04-26 00:00:00'), Timestamp('1990-04-27 00:00:00'), Timestamp('1990-04-28 00:00:00'), Timestamp('1990-04-29 00:00:00'), Timestamp('1990-04-30 00:00:00'), Timestamp('1990-05-01 00:00:00'), Timestamp('1990-05-02 00:00:00'), Timestamp('1990-05-03 00:00:00'), Timestamp('1990-05-04 00:00:00')] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m variable_ts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreamflow_mmd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitation_deficit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_sin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_cos\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmax_AWAP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmin_AWAP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m station \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA5040517\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m \u001b[43mcamels_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_val_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mstations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 118\u001b[0m, in \u001b[0;36mPrepareData.get_train_val_test\u001b[0;34m(self, source, stations, scaled, target, start, end, discard, train, test)\u001b[0m\n\u001b[1;32m    115\u001b[0m summary_source \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m source \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_data\u001b[38;5;241m.\u001b[39mcolumns)]\n\u001b[1;32m    116\u001b[0m timeseries_source \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m source \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_data\u001b[38;5;241m.\u001b[39mcolumns)]        \n\u001b[0;32m--> 118\u001b[0m all_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_timeseries_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeseries_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstations\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloc[start:end]\n\u001b[1;32m    119\u001b[0m n_rows_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_data)\n\u001b[1;32m    121\u001b[0m all_data_discarded \u001b[38;5;241m=\u001b[39m all_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mint\u001b[39m(n_rows_all\u001b[38;5;241m*\u001b[39mdiscard):]\n",
      "Cell \u001b[0;32mIn[47], line 80\u001b[0m, in \u001b[0;36mPrepareData.get_timeseries_data\u001b[0;34m(self, source, stations)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m column for station \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation_df[col]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing values. Filling with previous year data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m     null_data \u001b[38;5;241m=\u001b[39m station_df[col][(station_df[col]\u001b[38;5;241m.\u001b[39misna()) \u001b[38;5;241m&\u001b[39m (station_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m>\u001b[39m dt\u001b[38;5;241m.\u001b[39mdatetime(\u001b[38;5;241m1980\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m31\u001b[39m))]\n\u001b[0;32m---> 80\u001b[0m     station_df\u001b[38;5;241m.\u001b[39mloc[null_data\u001b[38;5;241m.\u001b[39mindex, col] \u001b[38;5;241m=\u001b[39m \u001b[43mstation_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnull_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffsets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m365\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m column for station \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation_df[col]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing values after filling with previous year data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_filtered\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexing.py:1041\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# we may have a nested tuples indexer here\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_nested_tuple_indexer(tup):\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_nested_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# we maybe be using a tuple to represent multiple dimensions here\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m ax0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     axis \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1153\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m axis \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# if we have a scalar, we are done\u001b[39;00m\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/srv/scratch/z5370003/miniconda3/envs/dl-env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[Timestamp('1990-02-20 00:00:00'), Timestamp('1990-02-21 00:00:00'), Timestamp('1990-02-22 00:00:00'), Timestamp('1990-02-23 00:00:00'), Timestamp('1990-02-24 00:00:00'), Timestamp('1990-02-25 00:00:00'), Timestamp('1990-02-26 00:00:00'), Timestamp('1990-02-27 00:00:00'), Timestamp('1990-02-28 00:00:00'), Timestamp('1990-03-01 00:00:00'), Timestamp('1990-03-02 00:00:00'), Timestamp('1990-03-03 00:00:00'), Timestamp('1990-03-04 00:00:00'), Timestamp('1990-03-05 00:00:00'), Timestamp('1990-03-06 00:00:00'), Timestamp('1990-03-07 00:00:00'), Timestamp('1990-03-08 00:00:00'), Timestamp('1990-03-09 00:00:00'), Timestamp('1990-03-10 00:00:00'), Timestamp('1990-03-11 00:00:00'), Timestamp('1990-03-12 00:00:00'), Timestamp('1990-03-13 00:00:00'), Timestamp('1990-03-14 00:00:00'), Timestamp('1990-03-15 00:00:00'), Timestamp('1990-03-16 00:00:00'), Timestamp('1990-03-17 00:00:00'), Timestamp('1990-03-18 00:00:00'), Timestamp('1990-03-19 00:00:00'), Timestamp('1990-03-20 00:00:00'), Timestamp('1990-03-21 00:00:00'), Timestamp('1990-03-22 00:00:00'), Timestamp('1990-03-23 00:00:00'), Timestamp('1990-03-24 00:00:00'), Timestamp('1990-03-25 00:00:00'), Timestamp('1990-03-26 00:00:00'), Timestamp('1990-03-27 00:00:00'), Timestamp('1990-03-28 00:00:00'), Timestamp('1990-03-29 00:00:00'), Timestamp('1990-03-30 00:00:00'), Timestamp('1990-03-31 00:00:00'), Timestamp('1990-04-01 00:00:00'), Timestamp('1990-04-02 00:00:00'), Timestamp('1990-04-03 00:00:00'), Timestamp('1990-04-04 00:00:00'), Timestamp('1990-04-05 00:00:00'), Timestamp('1990-04-06 00:00:00'), Timestamp('1990-04-07 00:00:00'), Timestamp('1990-04-08 00:00:00'), Timestamp('1990-04-09 00:00:00'), Timestamp('1990-04-10 00:00:00'), Timestamp('1990-04-11 00:00:00'), Timestamp('1990-04-12 00:00:00'), Timestamp('1990-04-13 00:00:00'), Timestamp('1990-04-14 00:00:00'), Timestamp('1990-04-15 00:00:00'), Timestamp('1990-04-16 00:00:00'), Timestamp('1990-04-17 00:00:00'), Timestamp('1990-04-18 00:00:00'), Timestamp('1990-04-19 00:00:00'), Timestamp('1990-04-20 00:00:00'), Timestamp('1990-04-21 00:00:00'), Timestamp('1990-04-22 00:00:00'), Timestamp('1990-04-23 00:00:00'), Timestamp('1990-04-24 00:00:00'), Timestamp('1990-04-25 00:00:00'), Timestamp('1990-04-26 00:00:00'), Timestamp('1990-04-27 00:00:00'), Timestamp('1990-04-28 00:00:00'), Timestamp('1990-04-29 00:00:00'), Timestamp('1990-04-30 00:00:00'), Timestamp('1990-05-01 00:00:00'), Timestamp('1990-05-02 00:00:00'), Timestamp('1990-05-03 00:00:00'), Timestamp('1990-05-04 00:00:00')] not in index\""
     ]
    }
   ],
   "source": [
    "variable_ts = ['streamflow_mmd', 'precipitation_deficit', 'year_sin', 'year_cos',\n",
    "               'tmax_AWAP', 'tmin_AWAP']\n",
    "\n",
    "station = 'A5040517'\n",
    "\n",
    "train_df, test_df = camels_data.get_train_val_test(source=variable_ts, \n",
    "                                            stations=[station])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04e650fb-6771-4fbe-98a4-0096a1f9a4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">A5040517</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>precipitation_deficit</th>\n",
       "      <th>streamflow_mmd</th>\n",
       "      <th>tmax_AWAP</th>\n",
       "      <th>tmin_AWAP</th>\n",
       "      <th>year_cos</th>\n",
       "      <th>year_sin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>-3.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.412000</td>\n",
       "      <td>8.383</td>\n",
       "      <td>0.999632</td>\n",
       "      <td>-0.027134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>-2.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.572999</td>\n",
       "      <td>8.231</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>-0.009934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        A5040517                                      \\\n",
       "source     precipitation_deficit streamflow_mmd  tmax_AWAP tmin_AWAP   \n",
       "date                                                                   \n",
       "2013-12-30                 -3.64            NaN  26.412000     8.383   \n",
       "2013-12-31                 -2.37            NaN  33.572999     8.231   \n",
       "\n",
       "                                \n",
       "source      year_cos  year_sin  \n",
       "date                            \n",
       "2013-12-30  0.999632 -0.027134  \n",
       "2013-12-31  0.999951 -0.009934  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_data.data_filtered.loc[camels_data.data_filtered.index[-2:] - dt.timedelta(days=365)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68637eed-17f5-469c-95d3-95b61ffe9f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (dl-env)",
   "language": "python",
   "name": "conda_dl-env_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
